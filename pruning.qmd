---
title: "HPML final project: Optimizing ESM, a protein large language model"
author: "Vinay Swamy"
format: docx
bibliography: HPML_project.bib
csl: diabetologia.csl
---



```{python}
from pathlib import Path
import torch
import esm
import lightning.pytorch as pl
import time
import sys
import esm2
import numpy as np
import tqdm
from copy import deepcopy
import xgboost as xgb
import scipy as sp 
from esm_data import BatchFilesSeqDataset, ProtSeqBatchConverter, iter_manual_worker_init_fn
precision = 16
flash_attention = True

# ESM variables
model_name = "esm2_t33_650M_UR50D"
esm_pt = esm.pretrained.load_model_and_alphabet(model_name)

model_cache_dir = Path('/manitou/pmg/users/vss2134/cache/hub/checkpoints')
# data
fasta_files = [Path('/manitou/pmg/users/vss2134/HPML/project/fluorescence/fluorescence_train.fasta'),]  # worker=0 for single file

if precision == 16:
    autocast = True
    #assert flash_attention is True, 'FP16 wihtout flashattention gives poor results'
else:
    autocast = False
    assert flash_attention is False, 'FlashAttention only works with FP16/BF16'

model_location = model_cache_dir / f'{model_name}.pt'
model_data = torch.load(model_location, map_location="cpu")
model_args = model_data["cfg"]['model']
alphabet = esm.data.Alphabet.from_architecture("ESM-1b")

model_min_args = {
    # Model args
    'attention_heads': model_args.encoder_attention_heads,
    'embed_dim': model_args.encoder_embed_dim,
    'num_layers': model_args.encoder_layers,
    'token_dropout': model_args.token_dropout,
    'flash_attention': flash_attention,
    'alphabet': alphabet
}
model = esm2.ESM2(**model_min_args).to("cuda")
model_vanilla = deepcopy(model)
```

```{python}
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
import pandas as pd
from Bio.Seq import Seq
import torch 
train_seqs = []
val_seqs = []
seq_iter = SeqIO.parse("/manitou/pmg/users/vss2134/HPML/project/uniref50.fasta", "fasta")
for i in range(10000):
    fa_seq = next(seq_iter)
    seq = str(fa_seq.seq)
    if len(seq) > 1024:
        seq = seq=seq[:1024]
    if len(seq) > 100:
        train_seqs.append( SeqRecord(id=fa_seq.id, seq=Seq(seq), description="" ))    
for i in range(10000):
    fa_seq = next(seq_iter)
    seq = str(fa_seq.seq)
    if len(seq) > 1024:
        seq = seq=seq[:1024] 
    if len(seq) > 100:
        val_seqs.append( SeqRecord(id=fa_seq.id, seq=Seq(seq), description="" ))

# with open("prune_train.fasta", "w") as output_handle:
#     SeqIO.write(train_seqs, output_handle, "fasta")
# with open("prune_val.fasta", "w") as output_handle:
#     SeqIO.write(val_seqs, output_handle, "fasta")
```



```{python}

# train_ds = BatchFilesSeqDataset(
#     alphabet = alphabet,
#     files=[Path('/manitou/pmg/users/vss2134/HPML/project/esm_training/esm_training/prune_train.fasta'),],
#     toks_per_batch=toks_per_batch,  #(1024*5)4 highest for ESM1b 12GB
#     shuffle=False,
#     rand_crop_long=False,
#     drop_last=False,
#     skip_long=True,
#     max_tok_length=max_tok_length,
#     order_by_len=10000,
#     unnamed_fasta=True,  # fastas with number of sequences in name, useless feature
#     real_bos_eos=True
# )
class SeqDataSet:
    def __init__(self, seqs):
        self.seqs = seqs
    def __len__(self):
        return len(self.seqs)
    def __getitem__(self, idx):
        id = self.seqs[idx].id
        seq = str(self.seqs[idx].seq)
        return (id,seq,True,True)
train_ds = SeqDataSet(train_seqs)
val_ds = SeqDataSet(val_seqs)
train_dl = torch.utils.data.DataLoader(
    train_ds,
    num_workers=2,
    collate_fn=ProtSeqBatchConverter(
        alphabet,
        masking=True),
    batch_size=12,
    pin_memory=True,
    shuffle=True,
)
#train_dl = [batch for batch in train_dl]

val_dl = torch.utils.data.DataLoader(
    val_ds,
    num_workers=2,
    collate_fn=ProtSeqBatchConverter(
        alphabet,
        masking=True),
    batch_size=256,
    pin_memory=True,
    shuffle=False, 
    drop_last=False
)
#val_dl = [batch for batch in val_dl] # so masks are deterministic
```

```{python}
meltome_data = pd.read_csv("/manitou/pmg/users/vss2134/HPML/project/splits/mixed_split.csv").assign(
    seql = lambda x: x['sequence'].str.len()
).query("seql < 1024")

meltome_train_seqs = [SeqRecord(id=f"M{i}", seq=Seq(row['sequence']), description="") for i, row in meltome_data[meltome_data['set'] == 'train'].iterrows()]
meltome_train_dl = torch.utils.data.DataLoader(
    SeqDataSet(meltome_train_seqs),
    num_workers=2,
    collate_fn=ProtSeqBatchConverter(
        alphabet,
        masking=False),
    batch_size=64,
    pin_memory=True,
    shuffle=False,
)

meltome_test_seqs = [SeqRecord(id=f"M{i}", seq=Seq(row['sequence']), description="") for i, row in meltome_data[meltome_data['set'] == 'test'].iterrows()]
meltome_test_dl = torch.utils.data.DataLoader(
    SeqDataSet(meltome_test_seqs),
    num_workers=2,
    collate_fn=ProtSeqBatchConverter(
        alphabet,
        masking=False),
    batch_size=256,
    pin_memory=True,
    shuffle=False,
)
```



### vanilla, MLM eval 


```{python}

model.eval()
all_val_loss = []
with torch.no_grad():
    with torch.cuda.amp.autocast(enabled=autocast):
        for n, batch in enumerate( tqdm.tqdm(val_dl[:10])):
            (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
            noised_tokens = noised_tokens.to("cuda")
            lens = torch.tensor([len(s) for s in seq_str_list])
            lm_pred = model(noised_tokens)["logits"]
            masked_tokens = tokens.masked_fill(~noise_mask, -100)
            masked_tokens = masked_tokens.to("cuda")
            lm_loss_fn = torch.nn.CrossEntropyLoss(reduction='none',
                                                ignore_index=-100)
            loss_noreduce = lm_loss_fn(
            lm_pred.view(-1, 33),
            masked_tokens.view(-1)).float()
            loss_nonzero = loss_noreduce[masked_tokens.view(-1) != -100]
            all_val_loss.append(loss.detach().cpu().numpy())
            
```


### vanilla, FLIP-meltome_data

```{python}
meltome_train_emebeddings = []
with torch.no_grad():
    with torch.cuda.amp.autocast(enabled=autocast):
        for n, batch in enumerate( tqdm.tqdm(meltome_train_dl)):
            (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
            noised_tokens = noised_tokens.to("cuda")
            lens = torch.tensor([len(s) for s in seq_str_list])
            lm_pred = model_vanilla(noised_tokens, repr_layers=[33])["representations"][33]
            seq_lens = torch.tensor([len(s) for s in seq_str_list])
            for i in range(len(seq_lens)):
                meltome_train_emebeddings.append(lm_pred[i, :seq_lens[i], :].mean(0).cpu().numpy())

meltome_test_emebeddings = []
with torch.no_grad():
    with torch.cuda.amp.autocast(enabled=autocast):
        for n, batch in enumerate( tqdm.tqdm(meltome_test_dl)):
            (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
            noised_tokens = noised_tokens.to("cuda")
            lens = torch.tensor([len(s) for s in seq_str_list])
            lm_pred = model_vanilla(noised_tokens, repr_layers=[33])["representations"][33]
            seq_lens = torch.tensor([len(s) for s in seq_str_list])
            for i in range(len(seq_lens)):
                meltome_test_emebeddings.append(lm_pred[i, :seq_lens[i], :].mean(0).cpu().numpy())

meltome_train_mat = np.array(meltome_train_emebeddings)
meltome_test_mat = np.array(meltome_test_emebeddings)
meltome_train_labels = meltome_data[meltome_data['set'] == 'train']['target'].values
meltome_test_labels = meltome_data[meltome_data['set'] == 'test']['target'].values

regressor = xgb.XGBRegressor(device="cuda")
regressor.fit(meltome_train_mat, meltome_train_labels)
meltome_test_preds = regressor.predict(meltome_test_mat)

sp.stats.pearsonr(meltome_test_preds, meltome_test_labels)            

```


## Sparsify!


```{python}
from torch.sparse import to_sparse_semi_structured, SparseSemiStructuredTensor
SparseSemiStructuredTensor._FORCE_CUTLASS = True
from torch.ao.pruning import WeightNormSparsifier
sparsifier = WeightNormSparsifier(
    # apply sparsity to all blocks
    sparsity_level=1.0,
    # shape of 4 elemens is a block
    sparse_block_shape=(1, 4),
    # two zeros for every block of 4
    zeros_per_block=2)
sparse_config = [
    {"tensor_fqn": f"{fqn}.weight"}
    for fqn, module in model.named_modules()
    if isinstance(module, torch.nn.Linear) and "layer" in fqn
]
sparsifier.prepare(model, sparse_config)
sparsifier.step()
```



```{python}
import torch.nn.utils.prune as prune
import re 
def name2module(pl):
    name, param = pl 
    name = name.split(".")
    for i in range(len(name)):
        if re.match(r"\d+", name[i]):
            new_name = name[i-1] + f"[{name[i]}]"
            name[i-1] =  new_name
            name.pop(i)
            break 
    name = ".".join(name[:-1])
    return eval(f"model.{name}")

all_params = [i for i in model_vanilla.named_parameters() if "weight" in i[0]]
parameters_to_prune = [
    (name2module(i), "weight") for i in model.named_parameters() if "weight" in i[0]
]


prune.global_unstructured(
    parameters_to_prune,
    pruning_method=prune.L1Unstructured,
    amount=0.2,
)
```


### sparsified, FLIP-meltome_data

```{python}
meltome_train_emebeddings = []
with torch.no_grad():
    with torch.cuda.amp.autocast(enabled=autocast):
        for n, batch in enumerate( tqdm.tqdm(meltome_train_dl)):
            (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
            noised_tokens = noised_tokens.to("cuda")
            lens = torch.tensor([len(s) for s in seq_str_list])
            lm_pred = model(noised_tokens, repr_layers=[33])["representations"][33]
            seq_lens = torch.tensor([len(s) for s in seq_str_list])
            for i in range(len(seq_lens)):
                meltome_train_emebeddings.append(lm_pred[i, :seq_lens[i], :].mean(0).cpu().numpy())

meltome_test_emebeddings = []
with torch.no_grad():
    with torch.cuda.amp.autocast(enabled=autocast):
        for n, batch in enumerate( tqdm.tqdm(meltome_test_dl)):
            (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
            noised_tokens = noised_tokens.to("cuda")
            lens = torch.tensor([len(s) for s in seq_str_list])
            lm_pred = model(noised_tokens, repr_layers=[33])["representations"][33]
            seq_lens = torch.tensor([len(s) for s in seq_str_list])
            for i in range(len(seq_lens)):
                meltome_test_emebeddings.append(lm_pred[i, :seq_lens[i], :].mean(0).cpu().numpy())

meltome_train_mat = np.array(meltome_train_emebeddings)
meltome_test_mat = np.array(meltome_test_emebeddings)
meltome_train_labels = meltome_data[meltome_data['set'] == 'train']['target'].values
meltome_test_labels = meltome_data[meltome_data['set'] == 'test']['target'].values

regressor = xgb.XGBRegressor(device="cuda")
regressor.fit(meltome_train_mat, meltome_train_labels)
meltome_test_preds = regressor.predict(meltome_test_mat)

sp.stats.pearsonr(meltome_test_preds, meltome_test_labels)      

```


```{python}
model.eval()
all_val_loss_pruned_zeroshot = []
with torch.no_grad():
    with torch.cuda.amp.autocast(enabled=autocast):
        for n, batch in enumerate( tqdm.tqdm(val_dl[:10])):
            (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
            noised_tokens = noised_tokens.to("cuda")
            lens = torch.tensor([len(s) for s in seq_str_list])
            lm_pred = model(noised_tokens)["logits"]
            masked_tokens = tokens.masked_fill(~noise_mask, -100)
            masked_tokens = masked_tokens.to("cuda")
            lm_loss_fn = torch.nn.CrossEntropyLoss(reduction='none',
                                                ignore_index=-100)
            loss_noreduce = lm_loss_fn(
            lm_pred.view(-1, 33),
            masked_tokens.view(-1)).float()
            loss = loss_noreduce.sum() / noise_mask.sum()
            all_val_loss_pruned_zeroshot.append(loss.item())
```

```{python}

model_args = model_data["cfg"]['model']
betas = tuple([
            float(s) for s in model_args.adam_betas.strip('[|]').split(',')])
optimizer = torch.optim.AdamW(model.parameters(),  
                                     lr=0.0001,  
                                     betas=betas,
                                     weight_decay=model_args.weight_decay,
                                     eps=model_args.adam_eps,
                                    )
all_loss = []
nsample = 0
model.train()
with torch.cuda.amp.autocast(enabled=autocast):
    for n, batch in enumerate( tqdm.tqdm(train_dl)):
        (batch_labels, seq_str_list, noised_tokens, tokens, noise_mask, _) = batch
        noised_tokens = noised_tokens.to("cuda")
        lens = torch.tensor([len(s) for s in seq_str_list])
        lm_pred = model(noised_tokens)["logits"]
        masked_tokens = tokens.masked_fill(~noise_mask, -100)
        masked_tokens = masked_tokens.to("cuda")
        lm_loss_fn = torch.nn.CrossEntropyLoss(reduction='none',
                                            ignore_index=-100)
        loss_noreduce = lm_loss_fn(
        lm_pred.view(-1, 33),
        masked_tokens.view(-1)).float()
        loss = loss_noreduce.sum() / noise_mask.sum()
        loss.backward()
        if n %4 == 0: # gaccum 
            optimizer.step()
            optimizer.zero_grad()
        nsample += noised_tokens.shape[0]
        all_loss.append(loss.item())


```

