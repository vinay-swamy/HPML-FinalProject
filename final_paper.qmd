---
title: "HPML final project: Optimizing ESM, a protein large language model"
author: "Vinay Swamy"
format: docx
bibliography: HPML_project.bib
csl: diabetologia.csl
---

## Introduction

Large language models such as GPT-4 have revolutionized natural language processing, and used self supervised learning on the transfomer architecture to learn representations of language that can reach near-human level performance on a variety of tasks. In a similar vein,  protein large language models (pLLMs) are a class of machine leanring model trained on millions of proteins sequences, technically identical to natural language processing (NLP) models such as GPT-4. In a pLLM, the models learns over proteins which are a sequence of amino acids, similar to how a sentence is a sequence of words. These models have been shown to be state-of-the-art for a variety of protein property prediction tasks, including protein stability and protein function prediction. There is also significant interest in using representations from pLLMs for higher order,  "proteome" level property predictions. These tasks could be for example predicting how sensitive a cell is to a drug, or represnetation learning on bacterial genome contigs. In this setting each observation is a set of proteins of size 10 to 100000, with each protein being anywhere from 100 to 10000 amino acids long. For a  set of $n$ proteins,  embeddings are generate from a frozen pLLM which are pooled along the sequence dimension to a vector with fixed length $d$ equal to the embedding dimension of the pLLM. The "proteome level" input to the downstream model is then an $ n x d $ matrix. Recent work has shown that directly finetuning a pLLM on these downstream tasks often leads to significant gains in peformance, at the cost of significant training overhead. However, directly finetuning a pLLM on a proteome level task has not yet been demonstrated. In this work, we explore the feasibility of finetuning a pLLM directly by applying several optimization techniques to reduce the memory overhead of finetuning a pLLM on extremely large batch size. We explores the following optimization techniques:
- FlashAttenion - a tiling based, IO-aware algorithm that is an exact replacement for the standard scaled dot product attention mechanism that requires significantly less memory than the standard implementation.
- Finetuning with Low Rank Adaption(LoRA) - freeze the weights of a model and instead use a pair of lowrank matrices to that are trainingable to fine tune with a reduced memory footprint.
- CPU offloading - When training, offload gradients and activation to CPU memory; this allows for lower GPU memory usage, but at the cost of slower training time.
- Pruning - Remove weights from the model with low numerical value to reduce model size. Pruned models can be accelerated using sparse tensor cores available on Ampere and newer Nvidia GPUs.

## Methods

We use the ESM2 family of pLLMs developed as the main models for thi project. ESM2 is masked-language modelling based pLLM trained on the Uniref50 dataset which contains 59 million protein sequences; ESM2 comes in 8M, 35M, 150M, and 650M parameter variants. We reimplement the ESM models in an new codebase to facilitate improvements. We use the orignal implementation of FlashAttention and LoRA provided by their respective authors. For CPU offloading we use the Microsoft DeepSpeed optimization framework in conjunction withe Pytorch-Lightning training framwork, which provides several easy-to-use methods for CPU offloading. Specifically, we use ZeRO stage 2 offload, which offloads optimizer state, gradients, and activations to CPU memory. For pruning experiments, we prune weights based on L1 norm, and where possible use the 2:4 semi sparse format in pytorch to reduce model size. To evaluate the effect of pruning on model performance, we evalaute pruned models on two tasks: masked language modelling using a subset of 1000 proteins from the uniref50 dataset, and performance on the FLIP-Meltome benchmark. This benchmark contains XXX proteins with known melting temperatures. On this dataset we freeze pruned ESM2 models, generate mean-pooled embeddings, and then train an xgboost regressor on these embeddings to predict melting temperature on a heldout test test, evaluating models by pearson correlation between observed and predicted. All experiments are run on a single A6000 GPU with 49GB of memory. 

## Results 

### FlashAttention Inference 

We first evaluate the effect of FlashAttention on the memory footprint of ESM2. To observed the benefits of FlashAttention, we profile the peak memory usage of the different sized ESM2 models, progressively increasing the length of the longest sequence while keep the total number of tokens fixed to 2560. We observe that FlashAttention greatly reduces memory usage, with a XXX gain for batches with a max sequence lenght of 2560. One key source of this memory reduciton is the fact that FlashAttention removes padding from seqeunces; in the standard implementation, padding is included in the attention computation so that attention matrix can be square. Removing this allows for a significant reduction in memory usage when we have batches that are highly hetergenous in length. 

Next, we evaluate calculate the maximum number of tokens that can be fit in memory for each model. We refer to this quantity as the maximum batchsize. In this experiment, we fix the maximum seequence length to 2560, and then randomly generate sequences of random lenght until the total number of tokens for a given batch is reached. We can see in XXX that FlashAttention scales linearly with respect to the number of tokens, but is significantly drops off as model size increases. 


### Finetuning optimizations

Given that we have greatly reduced the memory footprint w.r.t inference using FlashAttention, we next turn to see how we can reduce the footprint when finetuning. For this we use the LoRA finetuning algorithm, which uses a pair of low rank matrices to replace the weights of a model. In this setting we are only interested in the memory footprint while training. So similar to above, we generate batches of random proteins with random lengths until the desired total number of tokens is reached. We then increase the number of tokens until we reach the maximum number of tokens that can fit in memory. Table 1 shows the results of this experiment; We can see that direct finetuning of models dramatically reduces the maximum batch size by a factor of 10, compared to inference. Finetuning with LoRA does reduce the memory footprint, but the gains are proprtional to the size of the model. We then incorporate CPU offloading as another optimization. CPU offloading does allow us to significantly increase the maximum batch size, with the largest gains for the smallest model. 

### Pruning 

Pruning models involves setting parameters with low numerical value to 0. We evaluate both structured and unstructured L1 pruning. For structured pruning, we prune contiguous rows or columns that have the lowest L1 norm. For unstructured pruning, we prune individual weights with the lowest L1 norm. We evaluate the effect of pruning on the memory footprint of ESM2 models, and also evaluate the effect of pruning on model performance. For the latter, we evaluate the effect of pruning on masked language modelling performance, and also on the FLIP-Meltome benchmark. Strangely, we observe that pruning does not seem significantly effect model performance on the FLIP-Meltome benchmark even at high values of sparsity. This may mean that the FLIP Meltome task is a relatively simplistic task, and that further benchmarking is required on additional benchmarks. When evaluating on masked language modelling, we observe larger changes in performance; however these do not seem to correlate with the amount of pruning. These results as a whole are interesting and warrant further investigation. 

Given that the models were not sensitive to pruning, we then evaluated the use of 2:4 sparsity which can leverage sparse tensor cores on Ampere and newer Nvidia GPUs. In 2:4 sparsity, for every sequence of 4 paramters, 2 are set to 0, irrespective of their magnitude. In principle, use of sparse tensor cores should provide some speedup on the order of 25-30%, as well as a reduction in memory footprint. The current implementation in PyTorch is still experimental and only supports inference, so we we evalauted peformance by profiling the peak memory and run time when generating the embeddings for the FLIP-Meltome benchmark. We observe that 2:4 sparsity does provide a reduction in memory footprint of about XXX percent, but leads to a XXX longer run time. 

## Discussion

Collectively, these experiments demonstrate that by applying a variety of optimization techniques, we can significantly reduce the memory footprint of finetuning a pLLM on a proteome level task. We show that a combination of FlashAttention, LoRa, and CPU offloading collectively can improve the memory footprint of finetuning a model be a large factor. We also show that at least for the one benchmark we evaluated, pruning does not seem to significantly effect model performance. These results in particular, if consistent across mutliple benchmarks, could have significant implications for the use of pLLMs. This opens the door for fuuture work examining model distillation, where a sparsified large teacher model can be used to train a smaller student model, which while resource consumptive could lead to smaller models that are more amenable to proteome level task. An optimization technique we did not explore here but likely has significant potential is pipeline parallelism. In pipeline parallelism, the model is split across multiple GPUs,with each GPU computing a portion of the batch and parameter update. We chose not to expore this as it is technically challenging, but will be an interesting area of research in the future. In conclusion, we have shown that by applying a variety of optimization techniques, we can significantly reduce the memory 

